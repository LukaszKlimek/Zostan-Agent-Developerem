# ğŸ“ˆ Funkcje i Wykresy dla AI Engineering


---

## ğŸ¯ Po co mi to w AI?

| Gdzie w AI? | JakÄ… funkcjÄ™ tam zobaczysz? |
|-------------|----------------------------|
| Sieci neuronowe | Funkcje aktywacji (ReLU, sigmoid, tanh) |
| Trening modelu | Funkcja straty (loss function) |
| Klasyfikacja | Sigmoid zamieniajÄ…cy liczby na prawdopodobieÅ„stwa |
| Regresja liniowa | Prosta liniowa `y = ax + b` |
| Optymalizacja | Szukanie minimum funkcji straty |

**Funkcje = przepis na przeksztaÅ‚canie danych wejÅ›ciowych w wyjÅ›ciowe. To esencja kaÅ¼dego modelu ML!**

---

## 1ï¸âƒ£ CZYM JEST FUNKCJA?

### Definicja

Funkcja to **reguÅ‚a**, ktÃ³ra kaÅ¼dej wartoÅ›ci wejÅ›ciowej `x` przypisuje **dokÅ‚adnie jednÄ…** wartoÅ›Ä‡ wyjÅ›ciowÄ… `y`.

```
         FUNKCJA
x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ y
(wejÅ›cie)           (wyjÅ›cie)
```

Zapisujemy: `y = f(x)` lub `f: x â†’ y`

### PrzykÅ‚ady funkcji w AI

```python
# Regresja liniowa - prosta funkcja liniowa
def przewiduj_cene(powierzchnia):
    return 5000 * powierzchnia + 100000

# Sigmoid - zamienia dowolnÄ… liczbÄ™ na wartoÅ›Ä‡ 0..1
def sigmoid(x):
    return 1 / (1 + e^(-x))

# ReLU - jedna z najczÄ™stszych funkcji aktywacji
def relu(x):
    return max(0, x)
```

### Co to jest dziedzina i zbiÃ³r wartoÅ›ci?

```
Dziedzina (domain) = zbiÃ³r DOZWOLONYCH wartoÅ›ci x
ZbiÃ³r wartoÅ›ci (range) = zbiÃ³r MOÅ»LIWYCH wartoÅ›ci y

PrzykÅ‚ad: f(x) = âˆšx
  Dziedzina: x â‰¥ 0  (nie moÅ¼emy braÄ‡ pierwiastka z ujemnej)
  ZbiÃ³r wartoÅ›ci: y â‰¥ 0  (pierwiastek zawsze nieujemny)
```

---

## 2ï¸âƒ£ TYPY FUNKCJI WAÅ»NYCH W AI

### ğŸ“ Funkcja liniowa

```
f(x) = ax + b

gdzie:
  a = nachylenie (slope) â€” jak strome jest wzniesienie
  b = wyraz wolny (intercept) â€” gdzie linia przecina oÅ› Y
```

**Wykres:**
```
y
â”‚          /
â”‚         /  â† a = nachylenie (kÄ…t prostej)
â”‚        /
â”‚   b â†’ *    â† punkt przeciÄ™cia z osiÄ… Y
â”‚      /
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
```

**PrzykÅ‚ad:** model przewiduje cenÄ™ domu
```
cena = 5000 Ã— powierzchnia + 100000
        â†‘ a                    â†‘ b
   (5000 zÅ‚ za mÂ²)    (bazowa cena)
```

---

### ğŸ”” Funkcja kwadratowa

```
f(x) = axÂ² + bx + c
```

**Wykres (parabola):**
```
y
â”‚    *
â”‚   * *
â”‚  *   *
â”‚ *     *
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
  minimum â†‘
(gdy a > 0)
```

**W AI:** Funkcje straty (loss functions) czÄ™sto majÄ… ksztaÅ‚t paraboli â€” szukamy minimum!

---

### ğŸŒŠ Sigmoid â€” serce klasyfikatorÃ³w

```
           1
Ïƒ(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1 + e^(-x)
```

**Wykres:**
```
y
1.0 â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚        /
0.5 â”‚â”€â”€â”€â”€â”€â”€â”€*â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Ïƒ(0) = 0.5
    â”‚      /
0.0 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
     -5   -2   0   2    5
```

**WÅ‚aÅ›ciwoÅ›ci:**
- Zawsze zwraca wartoÅ›Ä‡ miÄ™dzy **0 a 1** âœ…
- Dla bardzo duÅ¼ych x â†’ 1 (prawie pewne)
- Dla bardzo maÅ‚ych x â†’ 0 (prawie niemoÅ¼liwe)
- Dla x = 0 â†’ 0.5 (brak pewnoÅ›ci)

**W AI:** Sigmoid zamienia wynik sieci neuronowej na **prawdopodobieÅ„stwo**.

```
Wynik sieci: 2.5  â†’  sigmoid(2.5) â‰ˆ 0.92  â†’  "92% pewnoÅ›ci klasy 1"
Wynik sieci: -1.0 â†’  sigmoid(-1.0) â‰ˆ 0.27 â†’  "27% pewnoÅ›ci klasy 1"
```

---

### âš¡ ReLU â€” najpopularniejsza funkcja aktywacji

```
ReLU(x) = max(0, x)
```

**Wykres:**
```
y
â”‚            /
â”‚           /
â”‚          /
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€*            â† f(x) = 0 dla x < 0
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
          0
```

**Prosto mÃ³wiÄ…c:**
- Liczby ujemne â†’ zamieÅ„ na 0
- Liczby dodatnie â†’ zostaw bez zmian

**Dlaczego tak popularna?**
```
Szybka do obliczenia: if x > 0 return x else return 0
Skuteczna w praktyce: eliminuje "niewaÅ¼ne" neurony
```

---

### ğŸŒ¡ï¸ Tanh â€” gdy potrzebujesz wartoÅ›ci -1 do 1

```
          e^x - e^(-x)
tanh(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          e^x + e^(-x)
```

**Wykres:**
```
y
 1.0 â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚        /
 0.0 â”‚â”€â”€â”€â”€â”€â”€â”€*â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† tanh(0) = 0
     â”‚      /
-1.0 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
```

Podobna do sigmoid, ale zakres to **-1 do 1** zamiast 0 do 1.

---

## 3ï¸âƒ£ ZÅOÅ»ENIE FUNKCJI

### Czym jest zÅ‚oÅ¼enie?

ZÅ‚oÅ¼enie to **funkcja wewnÄ…trz funkcji** â€” wynik jednej staje siÄ™ wejÅ›ciem drugiej.

```
f(g(x))  â†’  najpierw g, potem f
```

**PrzykÅ‚ad:**
```
g(x) = 2x + 1
f(x) = xÂ²

f(g(x)) = (2x + 1)Â²

Dla x = 3:
  g(3) = 2Ã—3 + 1 = 7
  f(7) = 7Â² = 49
```

### ZÅ‚oÅ¼enie w sieci neuronowej

SieÄ‡ neuronowa to **wielokrotne zÅ‚oÅ¼enie funkcji**:

```
WejÅ›cie â†’ [warstwa 1] â†’ [warstwa 2] â†’ [warstwa 3] â†’ WyjÅ›cie

x â†’ fâ‚(wâ‚Â·x + bâ‚) â†’ fâ‚‚(wâ‚‚Â·hâ‚ + bâ‚‚) â†’ fâ‚ƒ(wâ‚ƒÂ·hâ‚‚ + bâ‚ƒ) â†’ Å·
```

KaÅ¼da warstwa to: aktywacja(wagi Â· poprzednie_wyjÅ›cie + bias)

---

## 4ï¸âƒ£ ODCZYTYWANIE WYKRESÃ“W

### Co wykres mÃ³wi o funkcji?

```
                 PUNKT MAKSIMUM
                       â†“
y                      *
â”‚                    *   *
â”‚                  *       *
â”‚                *           *
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€*               *â”€â”€â”€â”€
â”‚            *                   *
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
            â†‘                    â†‘
       wzrost funkcji        spadek funkcji
```

**Co warto odczytaÄ‡:**
- **Minimum/maksimum** â€” gdzie funkcja osiÄ…ga ekstrema
- **PrzejÅ›cie przez zero** â€” gdzie `f(x) = 0`
- **MonotonicznoÅ›Ä‡** â€” czy roÅ›nie czy maleje
- **Zachowanie na kraÅ„cach** â€” co siÄ™ dzieje dla duÅ¼ych/maÅ‚ych x

### PrzykÅ‚ad: wykres funkcji straty podczas trenowania

```
Loss
â”‚\
â”‚ \
â”‚  \
â”‚   â”€â”€\
â”‚      â”€â”€\
â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epoki treningowe

Dobry trening: loss spada i stabilizuje siÄ™
```

---

## 5ï¸âƒ£ FUNKCJA WIELU ZMIENNYCH

W ML prawie zawsze mamy **wiele zmiennych wejÅ›ciowych**:

```
f(xâ‚, xâ‚‚, ..., xâ‚™) = wynik

PrzykÅ‚ad - przewidywanie ceny domu:
cena = f(powierzchnia, pokoje, dzielnica, rok_budowy)
     = 5000Ã—powierzchnia + 30000Ã—pokoje + 15000Ã—dzielnica - 1000Ã—wiek
```

**Wykres** takiej funkcji to powierzchnia w przestrzeni wielowymiarowej (trudna do wizualizacji, ale koncepcja ta sama).

---

## ğŸ“Œ Podsumowanie â€” Kluczowe Funkcje w AI

| Funkcja | WzÃ³r | Zastosowanie w AI |
|---------|------|-------------------|
| **Liniowa** | `y = ax + b` | Regresja liniowa, warstwy sieci |
| **Sigmoid** | `1/(1+e^(-x))` | Klasyfikacja binarna, wyjÅ›cie modelu |
| **ReLU** | `max(0, x)` | Funkcja aktywacji w sieciach neuronowych |
| **Tanh** | `(eË£-eâ»Ë£)/(eË£+eâ»Ë£)` | Warstwy ukryte LSTM/RNN |
| **Softmax** | `eË£áµ¢ / Î£eË£â±¼` | Klasyfikacja wieloklasowa (wyjÅ›cie) |
| **MSE (strata)** | `(y - Å·)Â²` | Funkcja straty w regresji |

---

## âœ… SprawdÅº siÄ™

1. âœ… Jakie wartoÅ›ci zwraca funkcja sigmoid?
2. âœ… Co robi ReLU z liczbÄ… ujemnÄ…?
3. âœ… Jak wyglÄ…da wykres funkcji liniowej z `a > 0`?
4. âœ… Dlaczego sigmoid nadaje siÄ™ do klasyfikacji?

<details>
<summary>Odpowiedzi</summary>

1. Sigmoid zwraca wartoÅ›ci z przedziaÅ‚u **(0, 1)** â€” zawsze
2. ReLU zamienia liczbÄ™ ujemnÄ… na **0**, dodatnie zostawia bez zmian
3. RosnÄ…ca linia prosta (im wiÄ™ksze x, tym wiÄ™ksze y)
4. PoniewaÅ¼ zwraca wartoÅ›ci 0-1, ktÃ³re moÅ¼na interpretowaÄ‡ jako **prawdopodobieÅ„stwo** przynaleÅ¼noÅ›ci do klasy

</details>

---

RozumiejÄ…c funkcje, moÅ¼esz teraz zrozumieÄ‡ jak modele ML **przeksztaÅ‚cajÄ… dane** i dlaczego wybÃ³r odpowiedniej funkcji aktywacji ma znaczenie dla dziaÅ‚ania sieci.
