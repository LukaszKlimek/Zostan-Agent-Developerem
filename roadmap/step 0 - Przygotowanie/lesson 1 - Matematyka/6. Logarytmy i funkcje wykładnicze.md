# ğŸ”¢ Logarytmy i Funkcje WykÅ‚adnicze dla AI Engineering


---

## ğŸ¯ Po co mi to w AI?

| Gdzie w AI? | Co tam robi logarytm / wykÅ‚adnicza? |
|-------------|-------------------------------------|
| Funkcja straty w klasyfikacji | Cross-entropy loss = -log(p) |
| Softmax | WykÅ‚adnicza eË£ w mianowniku |
| Learning rate | CzÄ™sto dobierany w skali logarytmicznej |
| Entropia informacji | H = -Î£ pÂ·log(p) |
| Skala decybeli i waÅ¼enia | Logarytmiczne skale w analizie |
| Ekspansja sieci | ZÅ‚oÅ¼onoÅ›Ä‡ obliczeniowa O(log n) |

**Logarytmy i wykÅ‚adnicze to matematyczny jÄ™zyk, ktÃ³rym AI opisuje niepewnoÅ›Ä‡ i prawdopodobieÅ„stwo!**

---

## 1ï¸âƒ£ FUNKCJA WYKÅADNICZA

### Definicja

```
f(x) = aË£    (gdzie a > 0, a â‰  1)

NajwaÅ¼niejsza: f(x) = eË£

gdzie e â‰ˆ 2.71828...  (liczba Eulera)
```

### Wykres

```
y
â”‚              *
â”‚            *
â”‚          *
â”‚        *
â”‚      *
â”‚    *
â”‚  *
â”‚ *
â”‚*
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
0  1  2  3  4  5

Funkcja roÅ›nie CORAZ SZYBCIEJ (wzrost wykÅ‚adniczy)
```

### WÅ‚aÅ›ciwoÅ›ci

```
eâ° = 1           â† zawsze
eÂ¹ = e â‰ˆ 2.718
eâ»Ë£ = 1/eË£      â† ujemna potÄ™ga = odwrotnoÅ›Ä‡
eË£ Â· eÊ¸ = eË£âºÊ¸  â† mnoÅ¼enie = dodawanie wykÅ‚adnikÃ³w
(eË£)Ê¸ = eË£Ê¸     â† potÄ™gowanie = mnoÅ¼enie wykÅ‚adnikÃ³w
```

### Gdzie eË£ pojawia siÄ™ w AI?

**Funkcja softmax** (klasyfikacja wieloklasowa):

```
          eË£áµ¢
P(i) = â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Î£â±¼ eË£Ê²

Dane wyjÅ›ciowe sieci: [2.0, 1.0, 0.1]

e^2.0 â‰ˆ 7.39
e^1.0 â‰ˆ 2.72
e^0.1 â‰ˆ 1.11

Suma = 11.22

P(klasa_0) = 7.39 / 11.22 â‰ˆ 0.659  (65.9%)
P(klasa_1) = 2.72 / 11.22 â‰ˆ 0.242  (24.2%)
P(klasa_2) = 1.11 / 11.22 â‰ˆ 0.099   (9.9%)

Suma = 100% âœ…
```

Softmax zamienia dowolne liczby na prawdopodobieÅ„stwa sumujÄ…ce siÄ™ do 1.

---

## 2ï¸âƒ£ LOGARYTM â€” ODWROTNOÅšÄ† WYKÅADNICZEJ

### Definicja

Logarytm odpowiada na pytanie: **"Do jakiej potÄ™gi podnieÅ›Ä‡ podstawÄ™, Å¼eby otrzymaÄ‡ x?"**

```
log_a(x) = y    âŸº    aÊ¸ = x

Czytamy: "logarytm x przy podstawie a"
```

**PrzykÅ‚ady:**
```
logâ‚‚(8) = 3     bo  2Â³ = 8
logâ‚â‚€(100) = 2  bo  10Â² = 100
logâ‚â‚€(1000) = 3 bo  10Â³ = 1000
logâ‚‚(1) = 0     bo  2â° = 1
```

### Typy logarytmÃ³w

```
logâ‚â‚€(x)  â€” logarytm dziesiÄ™tny (czÄ™sto pisany po prostu "log")
ln(x)     â€” logarytm naturalny (podstawa e)
logâ‚‚(x)   â€” logarytm binarny (popular w CS i informacji)

W ML i AI: najczÄ™Å›ciej uÅ¼ywamy ln(x)
```

### Wykres

```
y
â”‚
â”‚                     *****
â”‚                *****
â”‚           *****
â”‚      *****
â”‚  ****
â”‚**
â”‚*
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
0      1      2      3

log roÅ›nie CORAZ WOLNIEJ (odwrotnoÅ›Ä‡ wykÅ‚adniczej)
```

---

## 3ï¸âƒ£ WAÅ»NE PRAWA LOGARYTMÃ“W

Znanie tych praw pozwala uproÅ›ciÄ‡ skomplikowane wyraÅ¼enia.

### Prawo iloczyn â†’ suma

```
log(a Â· b) = log(a) + log(b)

PrzykÅ‚ad:
log(6) = log(2 Â· 3) = log(2) + log(3)
```

### Prawo iloraz â†’ rÃ³Å¼nica

```
log(a / b) = log(a) - log(b)

PrzykÅ‚ad:
log(5/2) = log(5) - log(2)
```

### Prawo potÄ™ga â†’ mnoÅ¼enie

```
log(aâ¿) = n Â· log(a)

PrzykÅ‚ad:
log(xÂ³) = 3 Â· log(x)
log(âˆšx) = (1/2) Â· log(x)
```

### Zamiana podstawy

```
           log_b(x)
log_a(x) = â”€â”€â”€â”€â”€â”€â”€â”€â”€
           log_b(a)

Np. logâ‚‚(x) = ln(x) / ln(2)
```

---

## 4ï¸âƒ£ LOGARYTMY W AI â€” PRAKTYCZNE ZASTOSOWANIA

### Cross-Entropy Loss â€” najwaÅ¼niejsza funkcja straty

Dla klasyfikacji binarnej:

```
L = -[y Â· log(Å·) + (1-y) Â· log(1-Å·)]

gdzie:
  y  = prawdziwa klasa (0 lub 1)
  Å·  = przewidziane prawdopodobieÅ„stwo

PrzykÅ‚ad:
  PrawidÅ‚owa klasa: y = 1
  Model przewiduje: Å· = 0.9   â†’ L = -log(0.9) â‰ˆ 0.105  (maÅ‚y bÅ‚Ä…d âœ…)
  Model przewiduje: Å· = 0.1   â†’ L = -log(0.1) â‰ˆ 2.303  (duÅ¼y bÅ‚Ä…d âŒ)
```

**Dlaczego logarytm?** Kara roÅ›nie BARDZO mocno gdy model jest pewny a siÄ™ myli:

```
Å· = 0.9  â†’ L â‰ˆ 0.1   â† maÅ‚y bÅ‚Ä…d gdy model ma racjÄ™
Å· = 0.5  â†’ L â‰ˆ 0.69
Å· = 0.1  â†’ L â‰ˆ 2.3   â† duÅ¼y bÅ‚Ä…d gdy model jest pewny ale siÄ™ myli
Å· = 0.01 â†’ L â‰ˆ 4.6   â† BARDZO duÅ¼y bÅ‚Ä…d â† mocna kara!
```

---

### Entropia informacji

```
H(p) = -Î£ p(x) Â· logâ‚‚(p(x))

Mierzy "iloÅ›Ä‡ niepewnoÅ›ci / informacji" w rozkÅ‚adzie.

PrzykÅ‚ad â€” rzut monetÄ…:
  Uczciwa moneta: P(orzeÅ‚) = P(reszka) = 0.5
  H = -(0.5Â·logâ‚‚(0.5) + 0.5Â·logâ‚‚(0.5)) = 1 bit  (maksymalna niepewnoÅ›Ä‡)

  SfaÅ‚szowana moneta: P(orzeÅ‚) = 0.99, P(reszka) = 0.01
  H â‰ˆ 0.08 bit  (maÅ‚a niepewnoÅ›Ä‡, prawie pewny wynik)
```

**W AI:** Entropia jest uÅ¼ywana w drzewach decyzyjnych (information gain) i mierzeniu jakoÅ›ci modeli.

---

### Skala logarytmiczna w praktyce

Wiele wielkoÅ›ci w ML wygodniej prezentowaÄ‡ w skali log:

```
Learning rate: 0.0001, 0.001, 0.01, 0.1
  Liniowo:     â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€ (trudno porÃ³wnaÄ‡ maÅ‚e wartoÅ›ci)
  Logarytmicznie: â”€â”€â”€â”€â”€*â”€â”€â”€â”€â”€*â”€â”€â”€â”€*â”€â”€â”€â”€*â”€â”€â”€ (rÃ³wne odstÄ™py)

Szukanie learning rate czÄ™sto przebiega w skali log!
```

---

## 5ï¸âƒ£ ZWIÄ„ZEK MIÄ˜DZY eË£ A ln(x)

SÄ… **funkcjami odwrotnymi**:

```
eË¡â¿â½Ë£â¾ = x
ln(eË£) = x

PrzykÅ‚ad:
  eÂ³ â‰ˆ 20.09
  ln(20.09) â‰ˆ 3   âœ…
```

**Wizualnie â€” symetria wzglÄ™dem prostej y = x:**

```
y
â”‚     *             â† eË£
â”‚    *
â”‚   *
â”‚  *             *  â† ln(x)
â”‚ *           *
â”‚*          *
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
       * â† punkt przeciÄ™cia (1,1)
```

---

## 6ï¸âƒ£ WYKÅADNICZY WZROST I ZANIK

### Wzrost wykÅ‚adniczy

```
f(t) = A Â· eáµáµ—    (k > 0 â†’ wzrost)

PrzykÅ‚ad: liczba uÅ¼ytkownikÃ³w: 1000 Â· e^(0.1Â·t)

t=0: 1000 uÅ¼ytkownikÃ³w
t=5: 1000 Â· e^0.5 â‰ˆ 1649 uÅ¼ytkownikÃ³w
t=10: 1000 Â· e^1 â‰ˆ 2718 uÅ¼ytkownikÃ³w
t=20: 1000 Â· e^2 â‰ˆ 7389 uÅ¼ytkownikÃ³w
```

### Zanik wykÅ‚adniczy (Exponential Decay)

```
f(t) = A Â· eâ»áµáµ—    (k > 0 â†’ zanik)

W AI: Learning rate schedule (cosine decay, exponential decay)

Learning rate maleje w czasie trenowania:
  Î·(t) = Î·â‚€ Â· e^(-Î»t)

Po wielu epokach: mniejsze kroki â†’ dokÅ‚adniejsza optymalizacja
```

---

## ğŸ“Œ Podsumowanie

| PojÄ™cie | Co to? | Po co w AI? |
|---------|--------|-------------|
| **eË£** | Funkcja wykÅ‚adnicza (podstawa e) | Softmax, sigmoid, funkcje aktywacji |
| **ln(x)** | Logarytm naturalny (podstawa e) | Cross-entropy loss, entropia |
| **log(aÂ·b)=log(a)+log(b)** | Iloczyn â†’ suma | Upraszczanie wyraÅ¼eÅ„ z prawdopod. |
| **log(aâ¿)=nÂ·log(a)** | PotÄ™ga â†’ mnoÅ¼nik | Upraszczanie obliczeÅ„ |
| **Cross-entropy** | -log(p) | Funkcja straty klasyfikatorÃ³w |
| **Entropia** | -Î£ pÂ·log(p) | Drzewa decyzyjne, miary niepewnoÅ›ci |
| **Softmax** | eË£áµ¢ / Î£eË£Ê² | WyjÅ›cie klasyfikatorÃ³w wieloklasowych |

---

## âœ… SprawdÅº siÄ™

1. âœ… Ile wynosi `logâ‚‚(32)`?
2. âœ… UproÅ›Ä‡ wyraÅ¼enie: `log(xÂ²) + log(y)`
3. âœ… Dlaczego cross-entropy loss uÅ¼ywa logarytmu zamiast np. kwadratu bÅ‚Ä™du?
4. âœ… Do czego sÅ‚uÅ¼y softmax i jakÄ… funkcjÄ™ matematycznÄ… wykorzystuje?

<details>
<summary>Odpowiedzi</summary>

1. `logâ‚‚(32) = 5`, bo `2âµ = 32`
2. `log(xÂ²) + log(y) = 2Â·log(x) + log(y) = log(xÂ²Â·y)` (prawo potÄ™gi + prawo iloczynu)
3. Logarytm bardzo mocno karze model gdy jest pewny a siÄ™ myli (np. przewiduje 1% szansy, a prawidÅ‚owa klasa to 1 â†’ log(0.01) â‰ˆ -4.6, czyli duÅ¼y bÅ‚Ä…d). Kwadrat bÅ‚Ä™du byÅ‚by Å‚agodniejszy.
4. Softmax zamienia wektor dowolnych liczb na **prawdopodobieÅ„stwa sumujÄ…ce siÄ™ do 1**. UÅ¼ywa funkcji wykÅ‚adniczej eË£ w liczniku i normalizuje przez sumÄ™ wszystkich eË£Ê².

</details>

---

Logarytmy i wykÅ‚adnicze sÄ… wszÄ™dzie tam, gdzie AI pracuje z prawdopodobieÅ„stwami. RozumiejÄ…c te funkcje, rozumiesz dlaczego modele klasyfikacyjne trenujÄ… siÄ™ tak jak siÄ™ trenujÄ…!
