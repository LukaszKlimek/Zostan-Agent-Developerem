# ğŸ² Teoria PrawdopodobieÅ„stwa dla AI Engineering


---

## ğŸ¯ Po co mi to w AI?

| Gdzie w AI? | Co tam robi prawdopodobieÅ„stwo? |
|-------------|--------------------------------|
| Klasyfikacja | Model zwraca *prawdopodobieÅ„stwo* klasy |
| Modele jÄ™zykowe | KaÅ¼dy token dobierany wedÅ‚ug rozkÅ‚adu p |
| Naiwny Bayes | CaÅ‚y klasyfikator oparty na tw. Bayesa |
| Dropout | Neurony wyÅ‚Ä…czane z prawdop. p |
| Metryki niepewnoÅ›ci | Jak pewny jest model swojej odpowiedzi? |

**Modele AI nie dajÄ… odpowiedzi "tak/nie" â€” dajÄ… *prawdopodobieÅ„stwa*!**

---

## 1ï¸âƒ£ PODSTAWY: CO TO JEST PRAWDOPODOBIEÅƒSTWO?

### Definicja

PrawdopodobieÅ„stwo to **liczba z przedziaÅ‚u [0, 1]**, ktÃ³ra mÃ³wi jak "moÅ¼liwe" jest dane zdarzenie.

```
P(zdarzenie) âˆˆ [0, 1]

P = 0   â†’ zdarzenie niemoÅ¼liwe
P = 0.5 â†’ zdarzenie rÃ³wnie moÅ¼liwe co niemoÅ¼liwe
P = 1   â†’ zdarzenie pewne
```

### PrzestrzeÅ„ prÃ³bna i zdarzenia

```
PrzykÅ‚ad: rzut kostkÄ… (6 Å›cianek)

PrzestrzeÅ„ prÃ³bna Î© = {1, 2, 3, 4, 5, 6}  â† wszystkie moÅ¼liwe wyniki

Zdarzenie A = "wypadÅ‚a parzysta" = {2, 4, 6}
P(A) = 3/6 = 0.5

Zdarzenie B = "wypadÅ‚a liczba > 4" = {5, 6}
P(B) = 2/6 â‰ˆ 0.33
```

### Podstawowe wÅ‚aÅ›ciwoÅ›ci

```
1. P(Î©) = 1              â† coÅ› ZAWSZE wypadnie
2. P(âˆ…) = 0              â† "nic" nigdy nie wypadnie
3. P(A) + P(nie-A) = 1   â† albo A, albo nie-A
4. P(A lub B) = P(A) + P(B) - P(A i B)
```

---

## 2ï¸âƒ£ PRAWDOPODOBIEÅƒSTWO WARUNKOWE

### Co to znaczy P(A|B)?

`P(A|B)` = prawdopodobieÅ„stwo zdarzenia A, **wiedzÄ…c Å¼e B juÅ¼ nastÄ…piÅ‚o**

```
P(A|B) = P(A i B) / P(B)
```

### PrzykÅ‚ad â€” spam filter

```
Zdarzenie A = "e-mail jest spamem"
Zdarzenie B = "e-mail zawiera sÅ‚owo GRATULACJE"

P(spam) = 0.3              â† 30% wszystkich maili to spam
P(GRATULACJE|spam) = 0.8   â† 80% spamu zawiera to sÅ‚owo
P(GRATULACJE|nie-spam) = 0.05  â† tylko 5% normalnych maili

Pytanie: dostaÅ‚em mail z "GRATULACJE" â€” jakie P(spam)?
```

---

## 3ï¸âƒ£ TWIERDZENIE BAYESA â€” SERCE WIELU ALGORYTMÃ“W ML

### WzÃ³r

```
                P(B|A) Â· P(A)
P(A|B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  P(B)
```

Gdzie:
- `P(A)` = **prior** â€” co wiemy *przed* obserwacjÄ…
- `P(B|A)` = **likelihood** â€” jak prawdopodobne jest B zakÅ‚adajÄ…c A
- `P(A|B)` = **posterior** â€” co wiemy *po* obserwacji B

### PrzykÅ‚ad â€” kontynuacja spam filtra

```
P(A) = P(spam) = 0.3
P(B|A) = P(GRATULACJE|spam) = 0.8
P(B) = P(GRATULACJE) = ?

P(B) = P(B|spam)Â·P(spam) + P(B|nie-spam)Â·P(nie-spam)
     = 0.8 Ã— 0.3 + 0.05 Ã— 0.7
     = 0.24 + 0.035 = 0.275

P(spam|GRATULACJE) = (0.8 Ã— 0.3) / 0.275
                   = 0.24 / 0.275
                   â‰ˆ 0.87

Wniosek: 87% szansy, Å¼e to spam!
```

### Intuicja Bayesa

```
PRIOR               +    OBSERWACJA     =    POSTERIOR
(co wiedzieliÅ›my)       (nowe dowody)        (co wiemy teraz)

"30% maili to spam" + "sÅ‚owo GRATULACJE" = "87% to spam"
```

---

## 4ï¸âƒ£ ROZKÅADY PRAWDOPODOBIEÅƒSTWA

### RozkÅ‚ad jednorodny (Uniform)

KaÅ¼da wartoÅ›Ä‡ rÃ³wnie prawdopodobna.

```
Rzut monetÄ…:
P(orzeÅ‚) = 0.5
P(reszka) = 0.5

Rzut kostkÄ…:
P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = 1/6
```

**W AI:** inicjalizacja wag modelu losowymi wartoÅ›ciami.

---

### RozkÅ‚ad Bernoulliego

Zdarzenie ma tylko dwa wyniki: sukces (1) lub poraÅ¼ka (0).

```
P(X = 1) = p
P(X = 0) = 1 - p

PrzykÅ‚ady:
  Rzut monetÄ…: p = 0.5
  Klasyfikator binarny: p = przewidywane prawdopodobieÅ„stwo
  Dropout: p = prawdopodobieÅ„stwo "wyÅ‚Ä…czenia" neuronu
```

---

### RozkÅ‚ad normalny (Gaussa) â€” NAJWAÅ»NIEJSZY!

```
KsztaÅ‚t: dzwon (bell curve)

          â†‘
          â”‚        *****
          â”‚      **     **
          â”‚    **         **
          â”‚  **             **
          â”‚**                 **
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    â†‘
                  Å›rednia Î¼

Parametry:
  Î¼ (mu) = Å›rednia â€” gdzie jest centrum dzwonu
  Ïƒ (sigma) = odchylenie standardowe â€” jak szeroki jest dzwon
```

**WÅ‚aÅ›ciwoÅ›ci:**
```
ReguÅ‚a 68-95-99.7:

â”‚â†â”€â”€â”€â”€ 68% â”€â”€â”€â”€â†’â”‚
â”‚â†â”€â”€â”€â”€â”€â”€ 95% â”€â”€â”€â”€â”€â”€â†’â”‚
â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€ 99.7% â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚

  Î¼-3Ïƒ  Î¼-2Ïƒ  Î¼-Ïƒ   Î¼   Î¼+Ïƒ  Î¼+2Ïƒ  Î¼+3Ïƒ
```

**W AI:**
- BÅ‚Ä™dy modelu czÄ™sto majÄ… rozkÅ‚ad normalny
- Inicjalizacja wag (np. inicjalizacja Glorot/He)
- ZaÅ‚oÅ¼enie w regresji liniowej

---

### RozkÅ‚ad Bernoulliego vs Normalny â€” kiedy ktÃ³ry?

```
Bernoulliego:
  "Czy e-mail to spam?" â†’ TAK/NIE
  "Czy klient kupi?" â†’ TAK/NIE

Normalny:
  "Ile waÅ¼y czÅ‚owiek?" â†’ ciÄ…gÅ‚a wartoÅ›Ä‡
  "Jaki bÄ™dzie bÅ‚Ä…d predykcji?" â†’ ciÄ…gÅ‚a wartoÅ›Ä‡
```

---

## 5ï¸âƒ£ WARTOÅšÄ† OCZEKIWANA I WARIANCJA

### WartoÅ›Ä‡ oczekiwana E[X]

"Jaka jest Å›rednia w dÅ‚ugim szeregu prÃ³b?"

```
Dyskretna: E[X] = Î£ xáµ¢ Â· P(X = xáµ¢)

PrzykÅ‚ad â€” rzut kostkÄ…:
E[X] = 1Ã—(1/6) + 2Ã—(1/6) + 3Ã—(1/6) + 4Ã—(1/6) + 5Ã—(1/6) + 6Ã—(1/6)
     = (1+2+3+4+5+6)/6 = 21/6 = 3.5
```

**W AI:** wartoÅ›Ä‡ oczekiwana straty = jak dobrze model radzi sobie "Å›rednio".

### Wariancja Var[X]

"Jak bardzo wartoÅ›ci odchylajÄ… siÄ™ od Å›redniej?"

```
Var[X] = E[(X - Î¼)Â²]

MaÅ‚a wariancja â†’ wartoÅ›ci skupione blisko Å›redniej
DuÅ¼a wariancja â†’ wartoÅ›ci mocno rozrzucone
```

---

## 6ï¸âƒ£ NIEZALEÅ»NOÅšÄ† ZDARZEÅƒ

### Kiedy zdarzenia sÄ… niezaleÅ¼ne?

A i B sÄ… **niezaleÅ¼ne** gdy:

```
P(A i B) = P(A) Â· P(B)

Inaczej: wiedza o B nic nam nie mÃ³wi o A.
```

**PrzykÅ‚ad:**
```
Rzut 1. kostkÄ… i rzut 2. kostkÄ… â€” niezaleÅ¼ne:
P(1. = 6 i 2. = 6) = 1/6 Ã— 1/6 = 1/36

Ale:
Deszcz i mokry chodnik â€” ZALEÅ»NE!
P(mokry chodnik | deszcz) â‰  P(mokry chodnik)
```

**W AI:** ZaÅ‚oÅ¼enie naiwnej niezaleÅ¼noÅ›ci jest fundamentem **Naiwnego Klasyfikatora Bayesa**.

---

## ğŸ“Œ Podsumowanie

| PojÄ™cie | Co to? | Po co w AI? |
|---------|--------|-------------|
| **PrawdopodobieÅ„stwo** | Liczba 0-1, miara moÅ¼liwoÅ›ci | WyjÅ›cie klasyfikatora |
| **P(A\|B)** | PrawdopodobieÅ„stwo warunkowe | Bayesowskie wnioskowanie |
| **Twierdzenie Bayesa** | Aktualizacja wiedzy z dowodem | Spam filter, Naiwny Bayes |
| **RozkÅ‚ad normalny** | Krzywa dzwonowa | Inicjalizacja wag, bÅ‚Ä™dy |
| **RozkÅ‚ad Bernoulliego** | Dwa wyniki: 0 lub 1 | Klasyfikacja binarna, dropout |
| **WartoÅ›Ä‡ oczekiwana** | Åšrednia waÅ¼ona | Funkcja straty, metryki |
| **Wariancja** | Rozrzut wokÃ³Å‚ Å›redniej | Regularyzacja, analiza modelu |

---

## âœ… SprawdÅº siÄ™

1. âœ… Co oznacza `P(A|B) = 0.8`?
2. âœ… Jakie sÄ… parametry rozkÅ‚adu normalnego?
3. âœ… Model klasyfikuje e-mail i zwraca 0.92 â€” co to znaczy?
4. âœ… Kiedy moÅ¼emy mnoÅ¼yÄ‡ prawdopodobieÅ„stwa: P(A i B) = P(A) Â· P(B)?

<details>
<summary>Odpowiedzi</summary>

1. PrawdopodobieÅ„stwo zajÅ›cia zdarzenia A, **wiedzÄ…c Å¼e B juÅ¼ nastÄ…piÅ‚o**, wynosi 80%
2. Åšrednia **Î¼** (centrum rozkÅ‚adu) i odchylenie standardowe **Ïƒ** (szerokoÅ›Ä‡ rozkÅ‚adu)
3. Model jest w **92% pewny**, Å¼e ten e-mail naleÅ¼y do klasy 1 (np. spam)
4. Tylko gdy A i B sÄ… **niezaleÅ¼ne** â€” wiedza o jednym nie zmienia prawdopodobieÅ„stwa drugiego

</details>

---

Teoria prawdopodobieÅ„stwa to jÄ™zyk, ktÃ³rym modele AI "mÃ³wiÄ…" o swoich przekonaniach. Zamiast twardych odpowiedzi, AI daje rozkÅ‚ady prawdopodobieÅ„stwa â€” a to jest znacznie bardziej uÅ¼yteczne i uczciwe!
